# ğŸ›¡ï¸ AI Audit Framework

*A Lightweight, End-to-End Toolkit for Auditing Text Classification Models*

---

## âœ¨ Overview

This framework provides a **pre-flight audit pipeline** for text classification models.
It detects **bias, robustness gaps, data leakage, redundancy, and fairness issues** before and after training, then produces a **single-page audit report** suitable for technical and non-technical reviewers.

Designed to be:

* **Practical** â€“ built on PyTorch, HuggingFace, scikit-learn
* **Lightweight** â€“ runs on CPU or GPU with simple commands
* **Transparent** â€“ outputs clear artifacts and summaries for governance
* **EB2-ready** â€“ produces `audit_summary.md` as a concise audit record

---

## ğŸ” Key Features

* **Dataset Health Checks**

  * Train/test split reproducibility
  * Similarity & duplicate detection (TF-IDF cosine)
  * Imbalance snapshot

* **Model Training**

  * Fast baseline: TF-IDF + Logistic Regression
  * Saved artifacts (`models/trained_model.pkl`, metrics, predictions)

* **Explainability**

  * SHAP global feature importances
  * LIME local interpretability

* **Bias & Fairness**

  * Identity & toxicity checks (HuggingFace models)
  * JSON/CSV summaries for review

* **Robustness**

  * Perturbation testing (typos, casing, whitespace, punctuation)
  * Flip-rate and confidence drop metrics

* **Repair Suggestions**

  * JSON plan with deduplication, rebalancing, and bias mitigation proposals

* **Unified Reporting**

  * Generates `reports/audit_summary.md` for decision-makers

---

## âš¡ Quickstart

### 1. Setup

```bash
# Clone & enter project
git clone <repo-url>
cd ai-audit-framework

# Create virtual environment (Python 3.12 recommended)
python -m venv .venv
.venv\Scripts\activate   # (Windows)
# source .venv/bin/activate  # (Linux/Mac)

# Install dependencies
pip install -r requirements.txt
```

### 2. Run the Pipeline

```bash
# Train/Test split
python src/modules/data_splitter.py --input data/raw/reviews.csv --text_col review --label_col sentiment

# Train baseline model
python src/modules/model_trainer.py --train data/processed/train.csv --test data/processed/test.csv --text_col review --label_col sentiment

# Run bias check
python src/modules/bias_detector.py --input data/processed/test.csv --text_col review

# Run robustness check
python src/modules/robustness_checker.py --model models/trained_model.pkl --data data/processed/test.csv --text_col review --label_col sentiment

# Generate audit report
python src/modules/report_builder.py --out reports/audit_summary.md
```

---

## ğŸ“‚ Project Structure

```
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/               # Original datasets
â”‚   â”œâ”€â”€ processed/         # Train/test splits
â”œâ”€â”€ models/                # Saved trained models
â”œâ”€â”€ reports/               # Bias, robustness, repair, and audit summaries
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ modules/           # Core audit modules
â”‚   â””â”€â”€ utils/             # Shared utilities
â””â”€â”€ requirements.txt       # Python dependencies
```

---

## ğŸ“‘ Output Artifacts

* **Model** â†’ `models/trained_model.pkl`
* **Metrics** â†’ `reports/metrics.json`
* **Bias Summary** â†’ `reports/bias_summary.json`
* **Robustness Summary** â†’ `reports/robustness_summary.json`
* **Repair Plan** â†’ `reports/repair_plan.json`
* **Final Audit** â†’ `reports/audit_summary.md`

---

## âœ… Example Use Case

* Train a sentiment analysis model on reviews
* Detect toxic/identity bias
* Test robustness against typos and noise
* Suggest repair strategies (e.g., rebalance, deduplication)
* Export a one-page `audit_summary.md` for governance

---

## ğŸ¤ Contributing

Pull requests and issue reports are welcome! This framework is designed to grow with community feedback.

---

## ğŸ“œ License

MIT License â€“ free to use, modify, and distribute.
